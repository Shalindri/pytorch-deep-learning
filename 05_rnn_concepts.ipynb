{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumani\n",
    "# 1-9-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Models in PyTorch: RNN, LSTM, and GRU\n",
    "\n",
    "This tutorial will cover the basics of sequential models in PyTorch, specifically focusing on Recurrent Neural Networks (RNNs), Long Short-Term Memory Networks (LSTMs), and Gated Recurrent Units (GRUs). We'll start with a brief overview of each model, followed by simple examples using tensors to illustrate their operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Sequential Models\n",
    "\n",
    "Sequential models are designed to process sequences of data, such as time series or natural language. Unlike feedforward neural networks, sequential models retain information from previous inputs in the sequence, making them suitable for tasks where the order of inputs matters.\n",
    "\n",
    "### 1.1 Recurrent Neural Networks (RNN)\n",
    "\n",
    "RNNs are a type of neural network designed to handle sequential data by maintaining a hidden state that captures information from previous steps in the sequence.\n",
    "\n",
    "### 1.2 Long Short-Term Memory Networks (LSTM)\n",
    "\n",
    "LSTMs are a special type of RNN designed to address the vanishing gradient problem. They use gates to control the flow of information, making them capable of learning long-term dependencies.\n",
    "\n",
    "### 1.3 Gated Recurrent Units (GRU)\n",
    "\n",
    "GRUs are similar to LSTMs but are simpler and faster to train. They combine the forget and input gates into a single update gate, reducing the number of parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch Implementation of RNN, LSTM, and GRU\n",
    "\n",
    "We'll implement each of these models using PyTorch and simple tensors to demonstrate how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input tensor (batch_size, sequence_length, input_size)\n",
    "input_seq = torch.tensor([[[0.1], [0.2], [0.3]]], dtype=torch.float32)\n",
    "\n",
    "# Define an RNN layer\n",
    "rnn = nn.RNN(input_size=1, hidden_size=2, batch_first=True)\n",
    "\n",
    "# Initialize the hidden state\n",
    "hidden_state = torch.zeros(1, 1, 2)\n",
    "\n",
    "# Forward pass through the RNN\n",
    "output, hidden_state = rnn(input_seq, hidden_state)\n",
    "\n",
    "# Print the output and hidden state\n",
    "print(\"RNN Output:\", output)\n",
    "print(\"RNN Hidden State:\", hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pass the same input again\n",
    "output, hidden_state = rnn(input_seq, hidden_state)\n",
    "\n",
    "# Print the output and hidden state\n",
    "print(\"RNN Output:\", output)\n",
    "print(\"RNN Hidden State:\", hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
