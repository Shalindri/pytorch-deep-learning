{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumani\n",
    "# 29-7-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network for MNIST Classification using PyTorch\n",
    "\n",
    "In this notebook, we will walk through the process of building, training, and evaluating a fully connected neural network for classifying handwritten digits from the MNIST dataset using PyTorch. We'll cover data loading and preprocessing, model definition, training, evaluation, and visualization of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Loading and Preprocessing\n",
    "\n",
    "We'll start by defining the transformations to apply to the images and loading the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the images to tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the pixel values with mean and std\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='.', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's visualize some sample images from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample images\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "for i in range(9):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    ax.imshow(example_data[i][0], cmap='gray')\n",
    "    ax.set_title(f'Label: {example_targets[i]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define the Neural Network Model\n",
    "\n",
    "We will define a simple fully connected neural network with two layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class FullyConnectedNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedNet, self).__init__()\n",
    "\n",
    "        # First Layer\n",
    "        self.layer_1 = nn.Linear(28*28, 512)\n",
    "        self.activation_1 = nn.ReLU() # ReLU activation\n",
    "\n",
    "        # Second layer\n",
    "        self.layer_2 = nn.Linear(512, 10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the image\n",
    "        x = x.view(-1, 28*28)\n",
    "\n",
    "        # Calling the first layer\n",
    "        x = self.layer_1(x)\n",
    "        x = self.activation_1(x)\n",
    "\n",
    "        # Calling the second layer\n",
    "        x = self.layer_2(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FullyConnectedNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Define the Loss Function and Optimizer\n",
    "\n",
    "We'll use cross-entropy loss and stochastic gradient descent (SGD) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Training the Model\n",
    "\n",
    "Let's define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate accuracy\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    return torch.sum(preds == labels).item() / len(labels)\n",
    "\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    global running_loss\n",
    "    global running_acc\n",
    "    running_acc = 0.0\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_acc += accuracy(outputs, labels)\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {i + 1}, Loss: {running_loss / 200:.4f}, Accuracy: {running_acc / 200:.4f}')\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "\n",
    "# Track training loss and accuracy\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accuracies.append(running_acc / len(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Evaluating the Model\n",
    "\n",
    "We'll define a function to evaluate the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test loop\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy(outputs, labels)\n",
    "            all_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    print(f'Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_acc / len(test_loader):.4f}')\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Test the model\n",
    "all_preds, all_labels = test(model, device, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Visualizing Training Progress\n",
    "\n",
    "Let's plot the training loss and accuracy over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and accuracy\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Confusion Matrix\n",
    "\n",
    "We'll plot the confusion matrix to see how well the model is performing across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[i for i in range(10)])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Visualizing Predictions\n",
    "\n",
    "Finally, let's visualize some sample predictions from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample images and predictions\n",
    "samples, labels = next(iter(test_loader))\n",
    "samples, labels = samples.to(device), labels.to(device)\n",
    "outputs = model(samples)\n",
    "_, preds = torch.max(outputs, 1)\n",
    "samples = samples.cpu().numpy()\n",
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(samples[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {labels[i]}, Prediction: {preds[i]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display some examples where the model made incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize incorrect predictions\n",
    "incorrect = [i for i in range(len(all_preds)) if all_preds[i] != all_labels[i]]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    idx = incorrect[i]\n",
    "    ax.imshow(test_loader.dataset[idx][0][0], cmap='gray')\n",
    "    ax.set_title(f'True: {all_labels[idx]}, Pred: {all_preds[idx]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Conclusion\n",
    "\n",
    "In this notebook, we have built a simple fully connected neural network for classifying MNIST digits using PyTorch. We covered data loading, preprocessing, model definition, training, evaluation, and visualization of results. This provides a good foundation for understanding the process of developing neural network models for image classification tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
